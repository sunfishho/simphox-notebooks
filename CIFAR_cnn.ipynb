{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9425075a-d65a-44a0-8daa-0a6d1cd730b0",
   "metadata": {},
   "source": [
    "# Scalability of *in situ* backpropagation\n",
    "\n",
    "In this notebook, we explore the scalability of *in situ* backpropagation as it pertains to the tradeoff between noise and energy efficiency and latency of photonic devices. \n",
    "- As far as scalability of the photonic advantage, we do our best to incorporate all of the different elements that contribute to the total energy consumption in the hybrid photonic neural network design, dominated by optoelectronic conversions and signal amplification, and any assumptions for this calculation are provided in the main text and/or Supplementary Material of the paper.\n",
    "- As far as noise error scaling, we explore the tradeoffs of various errors (e.g., systematic in the various photonic elements and random noise at the photodetector). We then perform large-scale simulations on MNIST data to show that realistic problems can be solved using our approach in the presence of error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f69805b-3389-4631-a951-df5ebcec6b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52cc7007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss.\n",
    "# print_every = 100\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caec0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the data\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f6003e7-2132-46d4-96bd-15332a3391a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0]\n",
    "    return x.view(N, -1)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd1991-ad92-4d4a-b221-d645fc676244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "def train(model, optimizer, epochs=10):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(trainloader):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy(testloader, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, channel_1, 5, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(channel_1, channel_2, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    Flatten(),\n",
    "    nn.Linear(channel_2 * 32 * 32, 10)\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef33ee-3536-4b00-86e4-acb3fcfc403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(trainloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32fef31b-333a-4824-8e95-f2ce59128b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "gray_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize the data\n",
    "    transforms.Grayscale(1)\n",
    "])\n",
    "\n",
    "gray_trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=gray_transform)\n",
    "gray_trainloader = torch.utils.data.DataLoader(gray_trainset, batch_size=64,\n",
    "                                          shuffle=True)\n",
    "\n",
    "gray_testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=gray_transform)\n",
    "gray_testloader = torch.utils.data.DataLoader(gray_testset, batch_size=64,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08d7465c-4ce1-4a0e-8e63-0f470e55bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray_check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on train set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "test_accuracies = []\n",
    "train_accuracies = []\n",
    "\n",
    "def gray_train(model, optimizer, epochs=10):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(gray_trainloader):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch {e}, loss = {loss.item()}')\n",
    "        test_accuracies.append(gray_check_accuracy(gray_testloader, model))\n",
    "        train_accuracies.append(gray_check_accuracy(gray_trainloader, model))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b84ca0b4-e220-4643-98ea-23b1df55fcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss = 2.080732822418213\n",
      "Checking accuracy on test set\n",
      "Got 2711 / 10000 correct (27.11)\n",
      "Checking accuracy on train set\n",
      "Got 13425 / 50000 correct (26.85)\n",
      "\n",
      "Epoch 1, loss = 2.053144693374634\n",
      "Checking accuracy on test set\n",
      "Got 3032 / 10000 correct (30.32)\n",
      "Checking accuracy on train set\n",
      "Got 15269 / 50000 correct (30.54)\n",
      "\n",
      "Epoch 2, loss = 1.9366974830627441\n",
      "Checking accuracy on test set\n",
      "Got 3369 / 10000 correct (33.69)\n",
      "Checking accuracy on train set\n",
      "Got 16829 / 50000 correct (33.66)\n",
      "\n",
      "Epoch 3, loss = 1.686638593673706\n",
      "Checking accuracy on test set\n",
      "Got 3531 / 10000 correct (35.31)\n",
      "Checking accuracy on train set\n",
      "Got 17857 / 50000 correct (35.71)\n",
      "\n",
      "Epoch 4, loss = 1.5496609210968018\n",
      "Checking accuracy on test set\n",
      "Got 3657 / 10000 correct (36.57)\n",
      "Checking accuracy on train set\n",
      "Got 18678 / 50000 correct (37.36)\n",
      "\n",
      "Epoch 5, loss = 1.577854871749878\n",
      "Checking accuracy on test set\n",
      "Got 3800 / 10000 correct (38.00)\n",
      "Checking accuracy on train set\n",
      "Got 19601 / 50000 correct (39.20)\n",
      "\n",
      "Epoch 6, loss = 1.6035006046295166\n",
      "Checking accuracy on test set\n",
      "Got 3920 / 10000 correct (39.20)\n",
      "Checking accuracy on train set\n",
      "Got 20237 / 50000 correct (40.47)\n",
      "\n",
      "Epoch 7, loss = 1.7436676025390625\n",
      "Checking accuracy on test set\n",
      "Got 3984 / 10000 correct (39.84)\n",
      "Checking accuracy on train set\n",
      "Got 21127 / 50000 correct (42.25)\n",
      "\n",
      "Epoch 8, loss = 1.8665742874145508\n",
      "Checking accuracy on test set\n",
      "Got 4088 / 10000 correct (40.88)\n",
      "Checking accuracy on train set\n",
      "Got 21785 / 50000 correct (43.57)\n",
      "\n",
      "Epoch 9, loss = 1.845494270324707\n",
      "Checking accuracy on test set\n",
      "Got 4162 / 10000 correct (41.62)\n",
      "Checking accuracy on train set\n",
      "Got 22415 / 50000 correct (44.83)\n",
      "\n",
      "Epoch 10, loss = 1.5800889730453491\n",
      "Checking accuracy on test set\n",
      "Got 4269 / 10000 correct (42.69)\n",
      "Checking accuracy on train set\n",
      "Got 23039 / 50000 correct (46.08)\n",
      "\n",
      "Epoch 11, loss = 1.5100892782211304\n",
      "Checking accuracy on test set\n",
      "Got 4286 / 10000 correct (42.86)\n",
      "Checking accuracy on train set\n",
      "Got 23238 / 50000 correct (46.48)\n",
      "\n",
      "Epoch 12, loss = 1.7393474578857422\n",
      "Checking accuracy on test set\n",
      "Got 4346 / 10000 correct (43.46)\n",
      "Checking accuracy on train set\n",
      "Got 23990 / 50000 correct (47.98)\n",
      "\n",
      "Epoch 13, loss = 1.8512753248214722\n",
      "Checking accuracy on test set\n",
      "Got 4372 / 10000 correct (43.72)\n",
      "Checking accuracy on train set\n",
      "Got 24581 / 50000 correct (49.16)\n",
      "\n",
      "Epoch 14, loss = 1.355989933013916\n",
      "Checking accuracy on test set\n",
      "Got 4413 / 10000 correct (44.13)\n",
      "Checking accuracy on train set\n",
      "Got 24911 / 50000 correct (49.82)\n",
      "\n",
      "Epoch 15, loss = 1.2928922176361084\n",
      "Checking accuracy on test set\n",
      "Got 4430 / 10000 correct (44.30)\n",
      "Checking accuracy on train set\n",
      "Got 25245 / 50000 correct (50.49)\n",
      "\n",
      "Epoch 16, loss = 1.2924079895019531\n",
      "Checking accuracy on test set\n",
      "Got 4470 / 10000 correct (44.70)\n",
      "Checking accuracy on train set\n",
      "Got 26025 / 50000 correct (52.05)\n",
      "\n",
      "Epoch 17, loss = 1.816136360168457\n",
      "Checking accuracy on test set\n",
      "Got 4514 / 10000 correct (45.14)\n",
      "Checking accuracy on train set\n",
      "Got 26639 / 50000 correct (53.28)\n",
      "\n",
      "Epoch 18, loss = 1.7440396547317505\n",
      "Checking accuracy on test set\n",
      "Got 4506 / 10000 correct (45.06)\n",
      "Checking accuracy on train set\n",
      "Got 26750 / 50000 correct (53.50)\n",
      "\n",
      "Epoch 19, loss = 1.3089637756347656\n",
      "Checking accuracy on test set\n",
      "Got 4547 / 10000 correct (45.47)\n",
      "Checking accuracy on train set\n",
      "Got 27423 / 50000 correct (54.85)\n",
      "\n",
      "Epoch 20, loss = 1.5956807136535645\n",
      "Checking accuracy on test set\n",
      "Got 4534 / 10000 correct (45.34)\n",
      "Checking accuracy on train set\n",
      "Got 28056 / 50000 correct (56.11)\n",
      "\n",
      "Epoch 21, loss = 1.112628698348999\n",
      "Checking accuracy on test set\n",
      "Got 4610 / 10000 correct (46.10)\n",
      "Checking accuracy on train set\n",
      "Got 28349 / 50000 correct (56.70)\n",
      "\n",
      "Epoch 22, loss = 1.5311814546585083\n",
      "Checking accuracy on test set\n",
      "Got 4609 / 10000 correct (46.09)\n",
      "Checking accuracy on train set\n",
      "Got 28957 / 50000 correct (57.91)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m fully_connected_model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m      4\u001b[0m     Flatten(),\n\u001b[1;32m      5\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(fully_connected_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, nesterov\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mgray_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfully_connected_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 37\u001b[0m, in \u001b[0;36mgray_train\u001b[0;34m(model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gray_trainloader):\n\u001b[0;32m---> 37\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# put model to training mode\u001b[39;00m\n\u001b[1;32m     38\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)  \u001b[38;5;66;03m# move to device, e.g. GPU\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[0;32m~/Photonic_computing/photonics_env/lib/python3.10/site-packages/torch/nn/modules/module.py:2269\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2266\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[1;32m   2267\u001b[0m                 \u001b[38;5;28;01myield\u001b[39;00m m\n\u001b[0;32m-> 2269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, mode: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sets the module in training mode.\u001b[39;00m\n\u001b[1;32m   2271\u001b[0m \n\u001b[1;32m   2272\u001b[0m \u001b[38;5;124;03m    This has any effect only on certain modules. See documentations of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   2283\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mode, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "fully_connected_model = nn.Sequential(\n",
    "    Flatten(),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 10)\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(fully_connected_model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "gray_train(fully_connected_model, optimizer, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1379b9-f31c-4054-816c-bf68e18ebbd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
