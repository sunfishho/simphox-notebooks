{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2db4e83-456f-407f-ac64-157edc026caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;21m2023-08-04 16:15:18,685 - butterfly_op.py[line:23] - WARNING: Import universal_cuda fail\u001b[0m\n",
      "\u001b[33;21m2023-08-04 16:15:18,686 - butterfly_op.py[line:27] - WARNING: Import hadamard_cuda fail\u001b[0m\n",
      "\u001b[33;21m2023-08-04 16:15:18,687 - matrix_parametrization.py[line:23] - WARNING: Cannot import matrix_parametrization_cuda. Decomposers can only work on CPU mode\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchonn as onn\n",
    "from torchonn.models import ONNBaseModel\n",
    "import torch.optim as optim\n",
    "from torchonn.op.mzi_op import project_matrix_to_unitary\n",
    "import torchvision.transforms as transforms\n",
    "import scipy.stats as stats\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1477c6-46df-4b7d-981f-41af759e8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniglotLoader():\n",
    "    def __init__(self, batch_size, n_way, k_spt, k_qry, downsampled_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        batchsize: imgs per batch\n",
    "        n_way: number of classes in support set\n",
    "        k_spt: number of images per class in support set\n",
    "        k_qry: number of images per class in query set\n",
    "        \"\"\"\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            lambda x: x.resize((downsampled_size, downsampled_size)),\n",
    "            lambda x: np.reshape(x, (downsampled_size, downsampled_size, 1)),\n",
    "            lambda x: (x - 128)/128\n",
    "        ])\n",
    "        \n",
    "        self.x_train = torchvision.datasets.Omniglot(root='./data',\n",
    "                                               download=True, transform=transform)\n",
    "        self.x_val = torchvision.datasets.Omniglot(root='./data', background = False,\n",
    "                                            download=True, transform=transform)\n",
    "        temp_train = {}\n",
    "        for (img, label) in self.x_train:\n",
    "            if label in temp_train.keys():\n",
    "                temp_train[label].append(img)\n",
    "            else:\n",
    "                temp_train[label] = [img]\n",
    "\n",
    "        temp_val = {}\n",
    "        for (img, label) in self.x_val:\n",
    "            if label in temp_val.keys():\n",
    "                temp_val[label].append(img)\n",
    "            else:\n",
    "                temp_val[label] = [img]\n",
    "\n",
    "        self.x = []\n",
    "        for label, imgs in temp_train.items():\n",
    "            self.x.append(np.array(imgs))\n",
    "        for label, imgs in temp_val.items():\n",
    "            self.x.append(np.array(imgs))\n",
    "\n",
    "        self.x = np.array(self.x).astype(np.float32)\n",
    "\n",
    "        temp = []\n",
    "        \n",
    "        self.x_train = self.x[:964]\n",
    "        self.x_test = self.x[964:]\n",
    "        self.num_classes = self.x.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.downsampled_size = downsampled_size\n",
    "        self.n_way = n_way\n",
    "        self.k_spt = k_spt\n",
    "        self.k_qry = k_qry\n",
    "        self.indexes = {\"train\": 0, \"test\": 0}\n",
    "        self.datasets = {\"train\": self.x_train, \"test\": self.x_test}\n",
    "\n",
    "        self.datasets_cache = {\"train\": self.load_data_cache(self.datasets[\"train\"]),\n",
    "                               \"test\": self.load_data_cache(self.datasets[\"test\"])}\n",
    "\n",
    "    def load_data_cache(self, data_pack):\n",
    "\n",
    "        num_episodes = 10\n",
    "        \n",
    "        set_size = self.k_spt * self.n_way\n",
    "        query_size = self.k_qry * self.n_way\n",
    "        data_cache = []\n",
    "\n",
    "        for sample in range(num_episodes):\n",
    "            x_supports, y_supports, x_queries, y_queries = [], [], [], []\n",
    "            for _ in range(self.batch_size):\n",
    "                x_support, y_support, x_query, y_query = [], [], [], []\n",
    "                selected_class = np.random.choice(data_pack.shape[0], self.n_way, False)\n",
    "                for j, cur_class in enumerate(selected_class):\n",
    "                    selected_image = np.random.choice(20, self.k_spt + self.k_qry, False)\n",
    "                    # meta-training and meta-test\n",
    "                    x_support.append(data_pack[cur_class][selected_image[:self.k_spt]])\n",
    "                    x_query.append(data_pack[cur_class][selected_image[self.k_spt:]])\n",
    "                    y_support.append([j for _ in range(self.k_spt)])\n",
    "                    y_query.append([j for _ in range(self.k_qry)])\n",
    "                    \n",
    "                support_perm = np.random.permutation(self.n_way * self.k_spt)\n",
    "                x_support = np.array(x_support).reshape(self.n_way * self.k_spt, 1, self.downsampled_size, self.downsampled_size)[support_perm]\n",
    "                y_support = np.array(y_support).reshape(self.n_way * self.k_spt)[support_perm]\n",
    "                query_perm = np.random.permutation(self.n_way * self.k_qry)\n",
    "                x_query = np.array(x_query).reshape(self.n_way * self.k_qry, 1, self.downsampled_size, self.downsampled_size)[query_perm]\n",
    "                y_query = np.array(y_query).reshape(self.n_way * self.k_qry)[query_perm]\n",
    "\n",
    "                x_supports.append(x_support)\n",
    "                y_supports.append(y_support)\n",
    "                x_queries.append(x_query)\n",
    "                y_queries.append(y_query)\n",
    "                \n",
    "            x_supports = np.array(x_supports).astype(np.float32).reshape(self.batch_size, set_size, 1, self.downsampled_size, self.downsampled_size)\n",
    "            y_supports = np.array(y_supports).astype(np.int32).reshape(self.batch_size, set_size)\n",
    "\n",
    "            x_queries = np.array(x_queries).astype(np.float32).reshape(self.batch_size, query_size, 1, self.downsampled_size, self.downsampled_size)\n",
    "            y_queries = np.array(y_queries).astype(np.int32).reshape(self.batch_size, query_size)\n",
    "\n",
    "            data_cache.append([x_supports, y_supports, x_queries, y_queries])\n",
    "            \n",
    "        return data_cache\n",
    "\n",
    "    def next(self, mode = \"train\"):\n",
    "        # update cache if needed\n",
    "        if self.indexes[mode] >= len(self.datasets_cache[mode]):\n",
    "            self.indexes[mode] = 0\n",
    "            self.datasets_cache[mode] = self.load_data_cache(self.datasets[mode])\n",
    "        next_batch = self.datasets_cache[mode][self.indexes[mode]]\n",
    "        self.indexes[mode] += 1\n",
    "        return next_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "38632d87-2e90-4e52-8856-be7ab5a76b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ONNModel(ONNBaseModel):\n",
    "    def __init__(self, device=torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "        self.conv0 = onn.layers.MZIBlockConv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=0,\n",
    "            dilation=1,\n",
    "            bias=False,\n",
    "            miniblock=1,\n",
    "            mode=\"usv\",\n",
    "            decompose_alg=\"clements\",\n",
    "            photodetect=True,\n",
    "            device=device,\n",
    "        )\n",
    "        self.conv1 = onn.layers.MZIBlockConv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=0,\n",
    "            dilation=1,\n",
    "            bias=False,\n",
    "            miniblock=1,\n",
    "            mode=\"usv\",\n",
    "            decompose_alg=\"clements\",\n",
    "            photodetect=True,\n",
    "            device=device,\n",
    "        )\n",
    "        self.conv2 = onn.layers.MZIBlockConv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=64, \n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=0,\n",
    "            dilation=1,\n",
    "            bias=False,\n",
    "            miniblock=1,\n",
    "            mode=\"usv\",\n",
    "            decompose_alg=\"clements\",\n",
    "            photodetect=True,\n",
    "            device=device,\n",
    "        )\n",
    "        self.conv3 = onn.layers.MZIBlockConv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=64,\n",
    "            kernel_size=2,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            dilation=1,\n",
    "            bias=False,\n",
    "            miniblock=1,\n",
    "            mode=\"usv\",\n",
    "            decompose_alg=\"clements\",\n",
    "            photodetect=True,\n",
    "            device=device,\n",
    "        )\n",
    "        self.linear = onn.layers.MZIBlockLinear(\n",
    "            in_features=64,\n",
    "            out_features=5, # because we're doing 5-way\n",
    "            bias=False,\n",
    "            miniblock=4,\n",
    "            mode=\"usv\",\n",
    "            decompose_alg=\"clements\",\n",
    "            photodetect=True,\n",
    "            device=device,\n",
    "        )\n",
    "        self.reset_all_parameters()\n",
    "\n",
    "        \n",
    "    def unitary_projection(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, onn.layers.MZIBlockLinear) or isinstance(m, onn.layers.MZIBlockConv2d):\n",
    "                m.U.data.copy_(project_matrix_to_unitary(m.U.data))\n",
    "                m.V.data.copy_(project_matrix_to_unitary(m.V.data))\n",
    "\n",
    "    # debugging the code\n",
    "    def reset_all_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, onn.layers.MZIBlockLinear):\n",
    "                m.reset_parameters()\n",
    "            if isinstance(m, onn.layers.MZIBlockConv2d):\n",
    "                W = torch.nn.init.kaiming_normal_(\n",
    "                    torch.empty(\n",
    "                        m.grid_dim_y,\n",
    "                        m.grid_dim_x,\n",
    "                        m.miniblock,\n",
    "                        m.miniblock,\n",
    "                        dtype=m.U.dtype,\n",
    "                        device=m.device,\n",
    "                    )\n",
    "                )\n",
    "                print(torch.svd(W, some = False))\n",
    "                U, S, V = torch.svd(W, some=False)\n",
    "                print(f\"W is: {W}\")\n",
    "                V = V.transpose(-2, -1)\n",
    "                m.U.data.copy_(U)\n",
    "                m.V.data.copy_(V)\n",
    "                m.S.data.copy_(torch.ones_like(S, device=m.device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv0(x))\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a325967d-916d-4349-9c53-b519936950c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm = 5\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# used to get rid of exploding gradients\n",
    "def normalize_grads(grad):\n",
    "    total_norm = torch.norm(torch.stack([torch.norm(g.detach()).to(device) for g in grad]))\n",
    "    clip_coef_clamped = torch.clamp(max_norm/(total_norm + 1e-6), max = 1.0)\n",
    "    for g in grad:\n",
    "        g.detach().mul_(clip_coef_clamped.to(g.device))\n",
    "\n",
    "class Meta(nn.Module):\n",
    "    # note that the maml-pytorch library uses an argparser to handle most of this instead of passing in individual parameters\n",
    "    def __init__(self, update_lr, meta_lr, n_way, k_spt, k_qry, task_num, update_step, update_step_test):\n",
    "        self.update_lr = update_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.n_way = n_way\n",
    "        self.k_spt = k_spt\n",
    "        self.k_qry = k_qry\n",
    "        self.task_num = task_num\n",
    "        self.update_step = update_step\n",
    "        self.update_step_test = update_step_test\n",
    "        \n",
    "        super(Meta, self).__init__()\n",
    "        self.net = ONNModel()\n",
    "        self.net.train()\n",
    "        self.meta_optim = optim.Adam(list(self.net.parameters()), lr=self.meta_lr)\n",
    "\n",
    "    def forward(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        self.net.unitary_projection()\n",
    "        y_spt = y_spt.to(torch.int64)\n",
    "        y_qry = y_qry.to(torch.int64)\n",
    "        task_num, setsz, c_, h, w = x_spt.size()\n",
    "        querysz = x_qry.size(1)\n",
    "\n",
    "        losses_q = [0 for _ in range(self.update_step + 1)]  # losses_q[i] is the loss on step i\n",
    "        corrects = [0 for _ in range(self.update_step + 1)]\n",
    "        for i in range(task_num):\n",
    "            # might need to clip grad norms here later\n",
    "\n",
    "            # calculate loss before any update\n",
    "            with torch.no_grad():\n",
    "                logits_q = self.net(x_qry[i])\n",
    "                loss_q = F.cross_entropy(logits_q, y_qry[i])\n",
    "                losses_q[0] += loss_q\n",
    "                pred_q = F.softmax(logits_q, dim = 1).argmax(dim = 1)\n",
    "                correct = torch.eq(pred_q, y_qry[i]).sum().item()\n",
    "                corrects[0] += correct\n",
    "                \n",
    "            for k in range(self.update_step):\n",
    "                logits = self.net(x_spt[i])\n",
    "                loss = F.cross_entropy(logits, y_spt[i])\n",
    "                print(f\"logits are: {logits}\")\n",
    "                print(f\"loss is: {loss}\")\n",
    "                grad = torch.autograd.grad(loss, list(self.net.parameters()), create_graph=True)\n",
    "                # avoid very large gradients killing everything\n",
    "                print(f\"grads are: {list(grad)}\")\n",
    "                normalize_grads(grad)\n",
    "                fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, list(self.net.parameters()))))\n",
    "                # set new net values\n",
    "                for l, param in enumerate(self.net.parameters()):\n",
    "                    param.data = nn.parameter.Parameter(torch.clone(fast_weights[l]))\n",
    "                \n",
    "                self.net.unitary_projection()\n",
    "                # apply to query\n",
    "                logits_q = self.net(x_qry[i])\n",
    "                loss_q = F.cross_entropy(logits_q, y_qry[i])\n",
    "                losses_q[k + 1] += loss_q\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pred_q = F.softmax(logits_q, dim = 1).argmax(dim = 1)\n",
    "                    correct = torch.eq(pred_q, y_qry[i]).sum().item()\n",
    "                    corrects[k + 1] += correct\n",
    "                    \n",
    "        loss_q = losses_q[-1] / task_num\n",
    "        self.meta_optim.zero_grad()\n",
    "        loss_q.retain_grad()\n",
    "        loss_q.backward()\n",
    "        self.meta_optim.step()\n",
    "        # is this needed?\n",
    "        self.net.unitary_projection()\n",
    "        accs = np.array(corrects) / (querysz * task_num)\n",
    "        return accs\n",
    "\n",
    "    def finetuning(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        y_spt = y_spt.to(torch.int64)\n",
    "        y_qry = y_qry.to(torch.int64)\n",
    "        querysz = x_qry.size(0)\n",
    "        corrects = [0 for _ in range(self.update_step_test + 1)]\n",
    "        net = deepcopy(self.net)\n",
    "        logits = net(x_spt)\n",
    "        loss = F.cross_entropy(logits, y_spt)\n",
    "        grad = torch.autograd.grad(loss, net.parameters())\n",
    "        # avoid very large gradients killing everything\n",
    "        normalize_grads(grad)\n",
    "        with torch.no_grad():\n",
    "            logits_q = net(x_qry)\n",
    "            pred_q = F.softmax(logits_q, dim = 1).argmax(dim = 1)\n",
    "            correct = torch.eq(pred_q, y_qry).sum().item()\n",
    "            corrects[0] += correct\n",
    "        for k in range(self.update_step_test):\n",
    "            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, list(net.parameters()))))\n",
    "            for l, param in enumerate(net.parameters()):\n",
    "                param.data = nn.parameter.Parameter(torch.clone(fast_weights[l]))\n",
    "            net.unitary_projection()\n",
    "            logits = net(x_spt)\n",
    "            loss = F.cross_entropy(logits, y_spt)\n",
    "            grad = torch.autograd.grad(loss, net.parameters())\n",
    "            normalize_grads(grad)\n",
    "            logits_q = net(x_qry)\n",
    "            loss_q = F.cross_entropy(logits_q, y_qry)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_q = F.softmax(logits_q, dim = 1).argmax(dim = 1)\n",
    "                correct = torch.eq(pred_q, y_qry).sum().item()\n",
    "                corrects[k + 1] += correct\n",
    "        del net\n",
    "        return np.array(corrects)/querysz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d134778-a40f-493a-a23d-c466a2eae973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "db_train = OmniglotLoader(batch_size = 32, n_way = 5, k_spt = 1, k_qry = 15, downsampled_size = 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7424c0bc-1d47-4af9-bdbf-5a124224f258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.svd(\n",
      "U=tensor([[[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]]]),\n",
      "S=tensor([[[0.4264],\n",
      "         [0.4294],\n",
      "         [0.3868],\n",
      "         [0.4883],\n",
      "         [0.8334],\n",
      "         [0.5137],\n",
      "         [0.4497],\n",
      "         [0.3658],\n",
      "         [0.3766]],\n",
      "\n",
      "        [[0.3224],\n",
      "         [0.4905],\n",
      "         [0.8362],\n",
      "         [0.1446],\n",
      "         [0.5300],\n",
      "         [0.1660],\n",
      "         [0.1474],\n",
      "         [0.2499],\n",
      "         [0.2588]],\n",
      "\n",
      "        [[0.2070],\n",
      "         [0.5448],\n",
      "         [0.0110],\n",
      "         [0.1192],\n",
      "         [0.1458],\n",
      "         [0.3111],\n",
      "         [0.8841],\n",
      "         [0.4205],\n",
      "         [0.3400]],\n",
      "\n",
      "        [[0.6623],\n",
      "         [0.5774],\n",
      "         [0.2307],\n",
      "         [0.0866],\n",
      "         [0.0944],\n",
      "         [0.3004],\n",
      "         [0.1425],\n",
      "         [0.0894],\n",
      "         [0.6374]],\n",
      "\n",
      "        [[0.5409],\n",
      "         [0.6189],\n",
      "         [0.5390],\n",
      "         [0.0637],\n",
      "         [0.4841],\n",
      "         [0.4985],\n",
      "         [0.5115],\n",
      "         [0.4023],\n",
      "         [0.3610]],\n",
      "\n",
      "        [[0.2410],\n",
      "         [0.4402],\n",
      "         [0.2602],\n",
      "         [0.1108],\n",
      "         [0.0566],\n",
      "         [0.5170],\n",
      "         [0.4683],\n",
      "         [0.4644],\n",
      "         [0.5601]],\n",
      "\n",
      "        [[0.8229],\n",
      "         [0.0790],\n",
      "         [0.7104],\n",
      "         [0.5611],\n",
      "         [0.2739],\n",
      "         [0.7978],\n",
      "         [0.1358],\n",
      "         [0.6187],\n",
      "         [0.0558]],\n",
      "\n",
      "        [[0.7049],\n",
      "         [0.1134],\n",
      "         [0.2020],\n",
      "         [0.7527],\n",
      "         [0.4442],\n",
      "         [0.4623],\n",
      "         [0.0511],\n",
      "         [0.3046],\n",
      "         [0.4829]],\n",
      "\n",
      "        [[0.2459],\n",
      "         [0.0572],\n",
      "         [0.0416],\n",
      "         [0.1605],\n",
      "         [0.1922],\n",
      "         [0.0973],\n",
      "         [0.2158],\n",
      "         [0.7565],\n",
      "         [0.2510]],\n",
      "\n",
      "        [[0.3808],\n",
      "         [0.5263],\n",
      "         [0.4647],\n",
      "         [0.8895],\n",
      "         [0.6671],\n",
      "         [0.2651],\n",
      "         [0.0936],\n",
      "         [0.5074],\n",
      "         [0.9061]],\n",
      "\n",
      "        [[0.6673],\n",
      "         [0.3560],\n",
      "         [0.3178],\n",
      "         [0.8667],\n",
      "         [0.3202],\n",
      "         [0.1804],\n",
      "         [0.4274],\n",
      "         [0.4725],\n",
      "         [0.1086]],\n",
      "\n",
      "        [[0.1310],\n",
      "         [0.5096],\n",
      "         [0.2587],\n",
      "         [0.6975],\n",
      "         [0.5678],\n",
      "         [0.1755],\n",
      "         [0.8003],\n",
      "         [0.5177],\n",
      "         [0.0950]],\n",
      "\n",
      "        [[0.3122],\n",
      "         [0.4756],\n",
      "         [0.3840],\n",
      "         [0.3738],\n",
      "         [0.1937],\n",
      "         [0.1718],\n",
      "         [0.0398],\n",
      "         [1.2566],\n",
      "         [0.2140]],\n",
      "\n",
      "        [[0.0365],\n",
      "         [0.1094],\n",
      "         [0.4344],\n",
      "         [0.1983],\n",
      "         [0.1560],\n",
      "         [0.2094],\n",
      "         [0.7002],\n",
      "         [0.3411],\n",
      "         [0.4064]],\n",
      "\n",
      "        [[0.7421],\n",
      "         [0.0671],\n",
      "         [0.0561],\n",
      "         [0.5503],\n",
      "         [0.6362],\n",
      "         [1.2246],\n",
      "         [0.1615],\n",
      "         [0.3266],\n",
      "         [0.4517]],\n",
      "\n",
      "        [[0.1214],\n",
      "         [0.2386],\n",
      "         [0.7825],\n",
      "         [0.0831],\n",
      "         [0.8732],\n",
      "         [0.4266],\n",
      "         [0.1567],\n",
      "         [0.3459],\n",
      "         [0.8818]],\n",
      "\n",
      "        [[0.4875],\n",
      "         [0.3684],\n",
      "         [0.5969],\n",
      "         [0.5802],\n",
      "         [0.3750],\n",
      "         [0.0255],\n",
      "         [0.6252],\n",
      "         [0.8574],\n",
      "         [0.1056]],\n",
      "\n",
      "        [[0.0974],\n",
      "         [0.0351],\n",
      "         [0.1145],\n",
      "         [0.6161],\n",
      "         [0.1283],\n",
      "         [0.1806],\n",
      "         [0.6446],\n",
      "         [0.0350],\n",
      "         [0.6400]],\n",
      "\n",
      "        [[0.0400],\n",
      "         [0.5449],\n",
      "         [0.3111],\n",
      "         [0.1827],\n",
      "         [0.9511],\n",
      "         [0.5874],\n",
      "         [0.0491],\n",
      "         [0.4920],\n",
      "         [0.1004]],\n",
      "\n",
      "        [[0.2843],\n",
      "         [0.1288],\n",
      "         [0.6771],\n",
      "         [0.9501],\n",
      "         [0.0941],\n",
      "         [0.2313],\n",
      "         [0.2665],\n",
      "         [0.5375],\n",
      "         [0.4807]],\n",
      "\n",
      "        [[0.1232],\n",
      "         [0.4572],\n",
      "         [0.4108],\n",
      "         [0.3503],\n",
      "         [0.2045],\n",
      "         [0.1377],\n",
      "         [0.0429],\n",
      "         [0.2181],\n",
      "         [0.1272]],\n",
      "\n",
      "        [[0.5188],\n",
      "         [0.2427],\n",
      "         [0.0205],\n",
      "         [0.2078],\n",
      "         [0.2315],\n",
      "         [0.5242],\n",
      "         [0.0120],\n",
      "         [0.6006],\n",
      "         [0.8396]],\n",
      "\n",
      "        [[0.4798],\n",
      "         [0.0313],\n",
      "         [1.2280],\n",
      "         [0.0768],\n",
      "         [0.4529],\n",
      "         [0.1545],\n",
      "         [0.1977],\n",
      "         [0.1089],\n",
      "         [0.8506]],\n",
      "\n",
      "        [[0.7249],\n",
      "         [0.2780],\n",
      "         [0.3285],\n",
      "         [0.6207],\n",
      "         [0.0633],\n",
      "         [0.3129],\n",
      "         [0.4281],\n",
      "         [0.2170],\n",
      "         [0.1321]],\n",
      "\n",
      "        [[0.3746],\n",
      "         [0.4385],\n",
      "         [0.2851],\n",
      "         [0.7777],\n",
      "         [0.5466],\n",
      "         [0.0662],\n",
      "         [0.0161],\n",
      "         [0.1157],\n",
      "         [0.1109]],\n",
      "\n",
      "        [[0.8061],\n",
      "         [0.6240],\n",
      "         [0.4898],\n",
      "         [0.3689],\n",
      "         [0.6915],\n",
      "         [0.2699],\n",
      "         [0.2360],\n",
      "         [0.2455],\n",
      "         [0.6536]],\n",
      "\n",
      "        [[0.1187],\n",
      "         [0.0662],\n",
      "         [0.8655],\n",
      "         [0.1180],\n",
      "         [0.3393],\n",
      "         [0.6180],\n",
      "         [0.2589],\n",
      "         [0.3095],\n",
      "         [0.2260]],\n",
      "\n",
      "        [[0.0023],\n",
      "         [0.8750],\n",
      "         [0.1175],\n",
      "         [0.6709],\n",
      "         [0.2402],\n",
      "         [0.0101],\n",
      "         [0.3149],\n",
      "         [0.3055],\n",
      "         [0.8657]],\n",
      "\n",
      "        [[0.6427],\n",
      "         [0.6927],\n",
      "         [0.5448],\n",
      "         [0.1212],\n",
      "         [0.2199],\n",
      "         [0.3803],\n",
      "         [0.1558],\n",
      "         [0.3904],\n",
      "         [0.0895]],\n",
      "\n",
      "        [[0.5411],\n",
      "         [0.6246],\n",
      "         [0.3063],\n",
      "         [0.0888],\n",
      "         [0.2833],\n",
      "         [0.4159],\n",
      "         [0.3022],\n",
      "         [0.7123],\n",
      "         [0.3891]],\n",
      "\n",
      "        [[0.7346],\n",
      "         [0.3474],\n",
      "         [0.3639],\n",
      "         [0.2558],\n",
      "         [0.8402],\n",
      "         [0.4868],\n",
      "         [0.9886],\n",
      "         [0.7271],\n",
      "         [0.0563]],\n",
      "\n",
      "        [[0.2134],\n",
      "         [0.3843],\n",
      "         [1.0610],\n",
      "         [0.6828],\n",
      "         [0.2232],\n",
      "         [0.7744],\n",
      "         [0.4189],\n",
      "         [0.8793],\n",
      "         [0.2080]],\n",
      "\n",
      "        [[0.3301],\n",
      "         [0.0431],\n",
      "         [0.1898],\n",
      "         [0.3417],\n",
      "         [1.0673],\n",
      "         [0.1371],\n",
      "         [0.1707],\n",
      "         [0.8270],\n",
      "         [0.2907]],\n",
      "\n",
      "        [[0.2853],\n",
      "         [0.4524],\n",
      "         [0.1728],\n",
      "         [0.3037],\n",
      "         [0.6476],\n",
      "         [0.5926],\n",
      "         [0.2902],\n",
      "         [0.5206],\n",
      "         [0.0665]],\n",
      "\n",
      "        [[0.4954],\n",
      "         [0.1032],\n",
      "         [0.8279],\n",
      "         [0.8858],\n",
      "         [1.0159],\n",
      "         [0.1790],\n",
      "         [0.4863],\n",
      "         [0.1968],\n",
      "         [0.1514]],\n",
      "\n",
      "        [[0.1145],\n",
      "         [0.3366],\n",
      "         [0.6697],\n",
      "         [0.1217],\n",
      "         [0.1966],\n",
      "         [0.0085],\n",
      "         [0.1259],\n",
      "         [0.4487],\n",
      "         [0.3980]],\n",
      "\n",
      "        [[0.1455],\n",
      "         [0.4859],\n",
      "         [0.3846],\n",
      "         [1.0642],\n",
      "         [0.0523],\n",
      "         [0.0176],\n",
      "         [0.6885],\n",
      "         [0.1783],\n",
      "         [0.1746]],\n",
      "\n",
      "        [[0.1905],\n",
      "         [0.2293],\n",
      "         [0.2955],\n",
      "         [0.1491],\n",
      "         [0.2513],\n",
      "         [0.0502],\n",
      "         [0.3067],\n",
      "         [0.1093],\n",
      "         [0.3885]],\n",
      "\n",
      "        [[0.2266],\n",
      "         [0.4545],\n",
      "         [0.5640],\n",
      "         [0.5103],\n",
      "         [0.2465],\n",
      "         [0.9639],\n",
      "         [0.4245],\n",
      "         [0.4266],\n",
      "         [0.5369]],\n",
      "\n",
      "        [[0.3908],\n",
      "         [0.9467],\n",
      "         [0.1213],\n",
      "         [0.2436],\n",
      "         [0.0069],\n",
      "         [0.8717],\n",
      "         [0.2013],\n",
      "         [0.2744],\n",
      "         [0.1687]],\n",
      "\n",
      "        [[0.1880],\n",
      "         [0.3923],\n",
      "         [0.5278],\n",
      "         [0.5584],\n",
      "         [0.4820],\n",
      "         [0.1539],\n",
      "         [0.2396],\n",
      "         [0.2228],\n",
      "         [0.2352]],\n",
      "\n",
      "        [[0.4250],\n",
      "         [0.0445],\n",
      "         [0.1903],\n",
      "         [0.4388],\n",
      "         [0.6786],\n",
      "         [0.3648],\n",
      "         [0.5352],\n",
      "         [0.2713],\n",
      "         [0.3738]],\n",
      "\n",
      "        [[0.6621],\n",
      "         [0.7478],\n",
      "         [0.2228],\n",
      "         [0.1790],\n",
      "         [0.1454],\n",
      "         [0.0412],\n",
      "         [0.1560],\n",
      "         [0.4285],\n",
      "         [0.2314]],\n",
      "\n",
      "        [[0.8463],\n",
      "         [0.1954],\n",
      "         [0.3726],\n",
      "         [0.0096],\n",
      "         [0.1779],\n",
      "         [0.3722],\n",
      "         [0.2131],\n",
      "         [0.5906],\n",
      "         [0.5169]],\n",
      "\n",
      "        [[0.6083],\n",
      "         [0.2713],\n",
      "         [0.0522],\n",
      "         [0.2438],\n",
      "         [0.0036],\n",
      "         [0.4560],\n",
      "         [0.0693],\n",
      "         [0.1358],\n",
      "         [0.3626]],\n",
      "\n",
      "        [[0.1996],\n",
      "         [0.0967],\n",
      "         [0.2324],\n",
      "         [0.6500],\n",
      "         [0.6809],\n",
      "         [0.8633],\n",
      "         [0.3687],\n",
      "         [0.6552],\n",
      "         [0.3154]],\n",
      "\n",
      "        [[0.1492],\n",
      "         [0.7032],\n",
      "         [0.1206],\n",
      "         [0.0814],\n",
      "         [0.6275],\n",
      "         [0.4549],\n",
      "         [0.0999],\n",
      "         [0.2357],\n",
      "         [0.1237]],\n",
      "\n",
      "        [[0.8807],\n",
      "         [0.2690],\n",
      "         [0.6664],\n",
      "         [0.4411],\n",
      "         [0.3557],\n",
      "         [0.1657],\n",
      "         [0.4294],\n",
      "         [0.2870],\n",
      "         [0.1414]],\n",
      "\n",
      "        [[0.3084],\n",
      "         [0.7416],\n",
      "         [0.6159],\n",
      "         [0.5550],\n",
      "         [0.0296],\n",
      "         [0.4406],\n",
      "         [0.0784],\n",
      "         [0.4314],\n",
      "         [0.5243]],\n",
      "\n",
      "        [[0.7118],\n",
      "         [0.0889],\n",
      "         [0.0594],\n",
      "         [0.7816],\n",
      "         [0.3973],\n",
      "         [0.2098],\n",
      "         [0.1648],\n",
      "         [0.1050],\n",
      "         [0.1912]],\n",
      "\n",
      "        [[0.3019],\n",
      "         [0.0374],\n",
      "         [0.0604],\n",
      "         [0.3911],\n",
      "         [0.2257],\n",
      "         [0.6630],\n",
      "         [0.3593],\n",
      "         [0.3045],\n",
      "         [1.3150]],\n",
      "\n",
      "        [[0.1002],\n",
      "         [0.3461],\n",
      "         [0.2491],\n",
      "         [0.7853],\n",
      "         [0.0417],\n",
      "         [0.3374],\n",
      "         [0.5981],\n",
      "         [0.3692],\n",
      "         [0.1100]],\n",
      "\n",
      "        [[0.2952],\n",
      "         [0.7096],\n",
      "         [0.2045],\n",
      "         [0.1959],\n",
      "         [0.8778],\n",
      "         [0.0018],\n",
      "         [0.0015],\n",
      "         [0.9322],\n",
      "         [0.4896]],\n",
      "\n",
      "        [[0.2100],\n",
      "         [0.3776],\n",
      "         [0.5413],\n",
      "         [0.1131],\n",
      "         [0.0147],\n",
      "         [0.1869],\n",
      "         [0.4576],\n",
      "         [0.2328],\n",
      "         [0.2016]],\n",
      "\n",
      "        [[0.0548],\n",
      "         [0.4337],\n",
      "         [0.5420],\n",
      "         [0.2203],\n",
      "         [0.2060],\n",
      "         [0.3011],\n",
      "         [0.1631],\n",
      "         [0.2162],\n",
      "         [0.4331]],\n",
      "\n",
      "        [[0.0946],\n",
      "         [0.4173],\n",
      "         [0.5253],\n",
      "         [0.9660],\n",
      "         [0.8692],\n",
      "         [0.3033],\n",
      "         [0.4452],\n",
      "         [0.1285],\n",
      "         [0.1536]],\n",
      "\n",
      "        [[0.8046],\n",
      "         [0.4378],\n",
      "         [0.1646],\n",
      "         [0.3224],\n",
      "         [0.4402],\n",
      "         [0.2027],\n",
      "         [0.6452],\n",
      "         [0.0667],\n",
      "         [0.2677]],\n",
      "\n",
      "        [[0.4838],\n",
      "         [0.3389],\n",
      "         [0.4636],\n",
      "         [0.3308],\n",
      "         [0.4042],\n",
      "         [0.6269],\n",
      "         [0.0701],\n",
      "         [0.2141],\n",
      "         [0.7251]],\n",
      "\n",
      "        [[0.0279],\n",
      "         [0.0183],\n",
      "         [0.0998],\n",
      "         [0.0726],\n",
      "         [0.3346],\n",
      "         [0.6198],\n",
      "         [0.3257],\n",
      "         [0.2406],\n",
      "         [0.2342]],\n",
      "\n",
      "        [[0.4814],\n",
      "         [0.5217],\n",
      "         [0.1549],\n",
      "         [0.0256],\n",
      "         [0.0942],\n",
      "         [0.4108],\n",
      "         [0.0670],\n",
      "         [0.4704],\n",
      "         [0.1218]],\n",
      "\n",
      "        [[0.1969],\n",
      "         [0.4113],\n",
      "         [0.5353],\n",
      "         [0.4431],\n",
      "         [1.3322],\n",
      "         [0.2216],\n",
      "         [1.4128],\n",
      "         [0.0411],\n",
      "         [0.5733]],\n",
      "\n",
      "        [[0.2712],\n",
      "         [0.0996],\n",
      "         [1.2787],\n",
      "         [0.2626],\n",
      "         [0.0055],\n",
      "         [0.5102],\n",
      "         [0.3490],\n",
      "         [1.0544],\n",
      "         [0.7516]],\n",
      "\n",
      "        [[0.1986],\n",
      "         [0.1053],\n",
      "         [0.1021],\n",
      "         [0.7947],\n",
      "         [0.2471],\n",
      "         [0.6374],\n",
      "         [0.0243],\n",
      "         [0.2300],\n",
      "         [0.0546]],\n",
      "\n",
      "        [[0.0757],\n",
      "         [0.2937],\n",
      "         [0.6151],\n",
      "         [0.1530],\n",
      "         [0.6693],\n",
      "         [0.7502],\n",
      "         [0.0633],\n",
      "         [0.7654],\n",
      "         [0.2010]]]),\n",
      "V=tensor([[[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]]]))\n",
      "W is: tensor([[[[ 0.4264]],\n",
      "\n",
      "         [[-0.4294]],\n",
      "\n",
      "         [[ 0.3868]],\n",
      "\n",
      "         [[ 0.4883]],\n",
      "\n",
      "         [[ 0.8334]],\n",
      "\n",
      "         [[ 0.5137]],\n",
      "\n",
      "         [[ 0.4497]],\n",
      "\n",
      "         [[ 0.3658]],\n",
      "\n",
      "         [[ 0.3766]]],\n",
      "\n",
      "\n",
      "        [[[-0.3224]],\n",
      "\n",
      "         [[-0.4905]],\n",
      "\n",
      "         [[ 0.8362]],\n",
      "\n",
      "         [[ 0.1446]],\n",
      "\n",
      "         [[ 0.5300]],\n",
      "\n",
      "         [[-0.1660]],\n",
      "\n",
      "         [[ 0.1474]],\n",
      "\n",
      "         [[ 0.2499]],\n",
      "\n",
      "         [[ 0.2588]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2070]],\n",
      "\n",
      "         [[ 0.5448]],\n",
      "\n",
      "         [[-0.0110]],\n",
      "\n",
      "         [[ 0.1192]],\n",
      "\n",
      "         [[ 0.1458]],\n",
      "\n",
      "         [[-0.3111]],\n",
      "\n",
      "         [[-0.8841]],\n",
      "\n",
      "         [[ 0.4205]],\n",
      "\n",
      "         [[-0.3400]]],\n",
      "\n",
      "\n",
      "        [[[-0.6623]],\n",
      "\n",
      "         [[-0.5774]],\n",
      "\n",
      "         [[-0.2307]],\n",
      "\n",
      "         [[ 0.0866]],\n",
      "\n",
      "         [[ 0.0944]],\n",
      "\n",
      "         [[-0.3004]],\n",
      "\n",
      "         [[-0.1425]],\n",
      "\n",
      "         [[-0.0894]],\n",
      "\n",
      "         [[-0.6374]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5409]],\n",
      "\n",
      "         [[ 0.6189]],\n",
      "\n",
      "         [[-0.5390]],\n",
      "\n",
      "         [[-0.0637]],\n",
      "\n",
      "         [[ 0.4841]],\n",
      "\n",
      "         [[-0.4985]],\n",
      "\n",
      "         [[ 0.5115]],\n",
      "\n",
      "         [[-0.4023]],\n",
      "\n",
      "         [[-0.3610]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2410]],\n",
      "\n",
      "         [[ 0.4402]],\n",
      "\n",
      "         [[ 0.2602]],\n",
      "\n",
      "         [[-0.1108]],\n",
      "\n",
      "         [[-0.0566]],\n",
      "\n",
      "         [[ 0.5170]],\n",
      "\n",
      "         [[-0.4683]],\n",
      "\n",
      "         [[ 0.4644]],\n",
      "\n",
      "         [[ 0.5601]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8229]],\n",
      "\n",
      "         [[-0.0790]],\n",
      "\n",
      "         [[ 0.7104]],\n",
      "\n",
      "         [[-0.5611]],\n",
      "\n",
      "         [[-0.2739]],\n",
      "\n",
      "         [[ 0.7978]],\n",
      "\n",
      "         [[ 0.1358]],\n",
      "\n",
      "         [[-0.6187]],\n",
      "\n",
      "         [[ 0.0558]]],\n",
      "\n",
      "\n",
      "        [[[-0.7049]],\n",
      "\n",
      "         [[-0.1134]],\n",
      "\n",
      "         [[-0.2020]],\n",
      "\n",
      "         [[-0.7527]],\n",
      "\n",
      "         [[-0.4442]],\n",
      "\n",
      "         [[-0.4623]],\n",
      "\n",
      "         [[ 0.0511]],\n",
      "\n",
      "         [[-0.3046]],\n",
      "\n",
      "         [[-0.4829]]],\n",
      "\n",
      "\n",
      "        [[[-0.2459]],\n",
      "\n",
      "         [[ 0.0572]],\n",
      "\n",
      "         [[ 0.0416]],\n",
      "\n",
      "         [[-0.1605]],\n",
      "\n",
      "         [[ 0.1922]],\n",
      "\n",
      "         [[-0.0973]],\n",
      "\n",
      "         [[ 0.2158]],\n",
      "\n",
      "         [[-0.7565]],\n",
      "\n",
      "         [[-0.2510]]],\n",
      "\n",
      "\n",
      "        [[[-0.3808]],\n",
      "\n",
      "         [[-0.5263]],\n",
      "\n",
      "         [[ 0.4647]],\n",
      "\n",
      "         [[-0.8895]],\n",
      "\n",
      "         [[ 0.6671]],\n",
      "\n",
      "         [[-0.2651]],\n",
      "\n",
      "         [[-0.0936]],\n",
      "\n",
      "         [[ 0.5074]],\n",
      "\n",
      "         [[ 0.9061]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6673]],\n",
      "\n",
      "         [[ 0.3560]],\n",
      "\n",
      "         [[ 0.3178]],\n",
      "\n",
      "         [[ 0.8667]],\n",
      "\n",
      "         [[-0.3202]],\n",
      "\n",
      "         [[-0.1804]],\n",
      "\n",
      "         [[-0.4274]],\n",
      "\n",
      "         [[-0.4725]],\n",
      "\n",
      "         [[-0.1086]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1310]],\n",
      "\n",
      "         [[-0.5096]],\n",
      "\n",
      "         [[-0.2587]],\n",
      "\n",
      "         [[ 0.6975]],\n",
      "\n",
      "         [[ 0.5678]],\n",
      "\n",
      "         [[-0.1755]],\n",
      "\n",
      "         [[ 0.8003]],\n",
      "\n",
      "         [[ 0.5177]],\n",
      "\n",
      "         [[-0.0950]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3122]],\n",
      "\n",
      "         [[-0.4756]],\n",
      "\n",
      "         [[ 0.3840]],\n",
      "\n",
      "         [[ 0.3738]],\n",
      "\n",
      "         [[ 0.1937]],\n",
      "\n",
      "         [[ 0.1718]],\n",
      "\n",
      "         [[ 0.0398]],\n",
      "\n",
      "         [[ 1.2566]],\n",
      "\n",
      "         [[-0.2140]]],\n",
      "\n",
      "\n",
      "        [[[-0.0365]],\n",
      "\n",
      "         [[-0.1094]],\n",
      "\n",
      "         [[ 0.4344]],\n",
      "\n",
      "         [[ 0.1983]],\n",
      "\n",
      "         [[ 0.1560]],\n",
      "\n",
      "         [[-0.2094]],\n",
      "\n",
      "         [[-0.7002]],\n",
      "\n",
      "         [[-0.3411]],\n",
      "\n",
      "         [[ 0.4064]]],\n",
      "\n",
      "\n",
      "        [[[-0.7421]],\n",
      "\n",
      "         [[-0.0671]],\n",
      "\n",
      "         [[-0.0561]],\n",
      "\n",
      "         [[ 0.5503]],\n",
      "\n",
      "         [[ 0.6362]],\n",
      "\n",
      "         [[ 1.2246]],\n",
      "\n",
      "         [[-0.1615]],\n",
      "\n",
      "         [[ 0.3266]],\n",
      "\n",
      "         [[ 0.4517]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1214]],\n",
      "\n",
      "         [[ 0.2386]],\n",
      "\n",
      "         [[ 0.7825]],\n",
      "\n",
      "         [[-0.0831]],\n",
      "\n",
      "         [[-0.8732]],\n",
      "\n",
      "         [[-0.4266]],\n",
      "\n",
      "         [[ 0.1567]],\n",
      "\n",
      "         [[ 0.3459]],\n",
      "\n",
      "         [[-0.8818]]],\n",
      "\n",
      "\n",
      "        [[[-0.4875]],\n",
      "\n",
      "         [[ 0.3684]],\n",
      "\n",
      "         [[ 0.5969]],\n",
      "\n",
      "         [[ 0.5802]],\n",
      "\n",
      "         [[ 0.3750]],\n",
      "\n",
      "         [[ 0.0255]],\n",
      "\n",
      "         [[-0.6252]],\n",
      "\n",
      "         [[ 0.8574]],\n",
      "\n",
      "         [[ 0.1056]]],\n",
      "\n",
      "\n",
      "        [[[-0.0974]],\n",
      "\n",
      "         [[-0.0351]],\n",
      "\n",
      "         [[ 0.1145]],\n",
      "\n",
      "         [[ 0.6161]],\n",
      "\n",
      "         [[ 0.1283]],\n",
      "\n",
      "         [[ 0.1806]],\n",
      "\n",
      "         [[-0.6446]],\n",
      "\n",
      "         [[ 0.0350]],\n",
      "\n",
      "         [[-0.6400]]],\n",
      "\n",
      "\n",
      "        [[[-0.0400]],\n",
      "\n",
      "         [[-0.5449]],\n",
      "\n",
      "         [[ 0.3111]],\n",
      "\n",
      "         [[-0.1827]],\n",
      "\n",
      "         [[ 0.9511]],\n",
      "\n",
      "         [[ 0.5874]],\n",
      "\n",
      "         [[ 0.0491]],\n",
      "\n",
      "         [[ 0.4920]],\n",
      "\n",
      "         [[-0.1004]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2843]],\n",
      "\n",
      "         [[ 0.1288]],\n",
      "\n",
      "         [[ 0.6771]],\n",
      "\n",
      "         [[-0.9501]],\n",
      "\n",
      "         [[-0.0941]],\n",
      "\n",
      "         [[ 0.2313]],\n",
      "\n",
      "         [[ 0.2665]],\n",
      "\n",
      "         [[ 0.5375]],\n",
      "\n",
      "         [[-0.4807]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1232]],\n",
      "\n",
      "         [[-0.4572]],\n",
      "\n",
      "         [[-0.4108]],\n",
      "\n",
      "         [[ 0.3503]],\n",
      "\n",
      "         [[-0.2045]],\n",
      "\n",
      "         [[-0.1377]],\n",
      "\n",
      "         [[-0.0429]],\n",
      "\n",
      "         [[ 0.2181]],\n",
      "\n",
      "         [[-0.1272]]],\n",
      "\n",
      "\n",
      "        [[[-0.5188]],\n",
      "\n",
      "         [[ 0.2427]],\n",
      "\n",
      "         [[ 0.0205]],\n",
      "\n",
      "         [[-0.2078]],\n",
      "\n",
      "         [[ 0.2315]],\n",
      "\n",
      "         [[-0.5242]],\n",
      "\n",
      "         [[ 0.0120]],\n",
      "\n",
      "         [[-0.6006]],\n",
      "\n",
      "         [[-0.8396]]],\n",
      "\n",
      "\n",
      "        [[[-0.4798]],\n",
      "\n",
      "         [[ 0.0313]],\n",
      "\n",
      "         [[-1.2280]],\n",
      "\n",
      "         [[ 0.0768]],\n",
      "\n",
      "         [[ 0.4529]],\n",
      "\n",
      "         [[-0.1545]],\n",
      "\n",
      "         [[-0.1977]],\n",
      "\n",
      "         [[ 0.1089]],\n",
      "\n",
      "         [[-0.8506]]],\n",
      "\n",
      "\n",
      "        [[[-0.7249]],\n",
      "\n",
      "         [[-0.2780]],\n",
      "\n",
      "         [[-0.3285]],\n",
      "\n",
      "         [[-0.6207]],\n",
      "\n",
      "         [[-0.0633]],\n",
      "\n",
      "         [[ 0.3129]],\n",
      "\n",
      "         [[-0.4281]],\n",
      "\n",
      "         [[ 0.2170]],\n",
      "\n",
      "         [[ 0.1321]]],\n",
      "\n",
      "\n",
      "        [[[-0.3746]],\n",
      "\n",
      "         [[ 0.4385]],\n",
      "\n",
      "         [[ 0.2851]],\n",
      "\n",
      "         [[-0.7777]],\n",
      "\n",
      "         [[ 0.5466]],\n",
      "\n",
      "         [[-0.0662]],\n",
      "\n",
      "         [[ 0.0161]],\n",
      "\n",
      "         [[ 0.1157]],\n",
      "\n",
      "         [[-0.1109]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8061]],\n",
      "\n",
      "         [[ 0.6240]],\n",
      "\n",
      "         [[ 0.4898]],\n",
      "\n",
      "         [[ 0.3689]],\n",
      "\n",
      "         [[ 0.6915]],\n",
      "\n",
      "         [[ 0.2699]],\n",
      "\n",
      "         [[-0.2360]],\n",
      "\n",
      "         [[ 0.2455]],\n",
      "\n",
      "         [[ 0.6536]]],\n",
      "\n",
      "\n",
      "        [[[-0.1187]],\n",
      "\n",
      "         [[ 0.0662]],\n",
      "\n",
      "         [[ 0.8655]],\n",
      "\n",
      "         [[ 0.1180]],\n",
      "\n",
      "         [[-0.3393]],\n",
      "\n",
      "         [[-0.6180]],\n",
      "\n",
      "         [[-0.2589]],\n",
      "\n",
      "         [[-0.3095]],\n",
      "\n",
      "         [[-0.2260]]],\n",
      "\n",
      "\n",
      "        [[[-0.0023]],\n",
      "\n",
      "         [[ 0.8750]],\n",
      "\n",
      "         [[-0.1175]],\n",
      "\n",
      "         [[ 0.6709]],\n",
      "\n",
      "         [[ 0.2402]],\n",
      "\n",
      "         [[-0.0101]],\n",
      "\n",
      "         [[ 0.3149]],\n",
      "\n",
      "         [[-0.3055]],\n",
      "\n",
      "         [[ 0.8657]]],\n",
      "\n",
      "\n",
      "        [[[-0.6427]],\n",
      "\n",
      "         [[-0.6927]],\n",
      "\n",
      "         [[-0.5448]],\n",
      "\n",
      "         [[-0.1212]],\n",
      "\n",
      "         [[-0.2199]],\n",
      "\n",
      "         [[-0.3803]],\n",
      "\n",
      "         [[-0.1558]],\n",
      "\n",
      "         [[ 0.3904]],\n",
      "\n",
      "         [[-0.0895]]],\n",
      "\n",
      "\n",
      "        [[[-0.5411]],\n",
      "\n",
      "         [[-0.6246]],\n",
      "\n",
      "         [[ 0.3063]],\n",
      "\n",
      "         [[-0.0888]],\n",
      "\n",
      "         [[-0.2833]],\n",
      "\n",
      "         [[-0.4159]],\n",
      "\n",
      "         [[-0.3022]],\n",
      "\n",
      "         [[ 0.7123]],\n",
      "\n",
      "         [[-0.3891]]],\n",
      "\n",
      "\n",
      "        [[[-0.7346]],\n",
      "\n",
      "         [[ 0.3474]],\n",
      "\n",
      "         [[-0.3639]],\n",
      "\n",
      "         [[-0.2558]],\n",
      "\n",
      "         [[-0.8402]],\n",
      "\n",
      "         [[-0.4868]],\n",
      "\n",
      "         [[ 0.9886]],\n",
      "\n",
      "         [[ 0.7271]],\n",
      "\n",
      "         [[-0.0563]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2134]],\n",
      "\n",
      "         [[-0.3843]],\n",
      "\n",
      "         [[ 1.0610]],\n",
      "\n",
      "         [[ 0.6828]],\n",
      "\n",
      "         [[ 0.2232]],\n",
      "\n",
      "         [[ 0.7744]],\n",
      "\n",
      "         [[ 0.4189]],\n",
      "\n",
      "         [[ 0.8793]],\n",
      "\n",
      "         [[-0.2080]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3301]],\n",
      "\n",
      "         [[-0.0431]],\n",
      "\n",
      "         [[ 0.1898]],\n",
      "\n",
      "         [[ 0.3417]],\n",
      "\n",
      "         [[ 1.0673]],\n",
      "\n",
      "         [[ 0.1371]],\n",
      "\n",
      "         [[-0.1707]],\n",
      "\n",
      "         [[ 0.8270]],\n",
      "\n",
      "         [[-0.2907]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2853]],\n",
      "\n",
      "         [[ 0.4524]],\n",
      "\n",
      "         [[ 0.1728]],\n",
      "\n",
      "         [[-0.3037]],\n",
      "\n",
      "         [[ 0.6476]],\n",
      "\n",
      "         [[-0.5926]],\n",
      "\n",
      "         [[-0.2902]],\n",
      "\n",
      "         [[-0.5206]],\n",
      "\n",
      "         [[-0.0665]]],\n",
      "\n",
      "\n",
      "        [[[-0.4954]],\n",
      "\n",
      "         [[ 0.1032]],\n",
      "\n",
      "         [[-0.8279]],\n",
      "\n",
      "         [[-0.8858]],\n",
      "\n",
      "         [[-1.0159]],\n",
      "\n",
      "         [[ 0.1790]],\n",
      "\n",
      "         [[ 0.4863]],\n",
      "\n",
      "         [[-0.1968]],\n",
      "\n",
      "         [[ 0.1514]]],\n",
      "\n",
      "\n",
      "        [[[-0.1145]],\n",
      "\n",
      "         [[-0.3366]],\n",
      "\n",
      "         [[ 0.6697]],\n",
      "\n",
      "         [[-0.1217]],\n",
      "\n",
      "         [[ 0.1966]],\n",
      "\n",
      "         [[-0.0085]],\n",
      "\n",
      "         [[ 0.1259]],\n",
      "\n",
      "         [[-0.4487]],\n",
      "\n",
      "         [[ 0.3980]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1455]],\n",
      "\n",
      "         [[-0.4859]],\n",
      "\n",
      "         [[-0.3846]],\n",
      "\n",
      "         [[ 1.0642]],\n",
      "\n",
      "         [[ 0.0523]],\n",
      "\n",
      "         [[-0.0176]],\n",
      "\n",
      "         [[-0.6885]],\n",
      "\n",
      "         [[-0.1783]],\n",
      "\n",
      "         [[ 0.1746]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1905]],\n",
      "\n",
      "         [[-0.2293]],\n",
      "\n",
      "         [[ 0.2955]],\n",
      "\n",
      "         [[-0.1491]],\n",
      "\n",
      "         [[-0.2513]],\n",
      "\n",
      "         [[-0.0502]],\n",
      "\n",
      "         [[-0.3067]],\n",
      "\n",
      "         [[ 0.1093]],\n",
      "\n",
      "         [[ 0.3885]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2266]],\n",
      "\n",
      "         [[-0.4545]],\n",
      "\n",
      "         [[-0.5640]],\n",
      "\n",
      "         [[-0.5103]],\n",
      "\n",
      "         [[ 0.2465]],\n",
      "\n",
      "         [[ 0.9639]],\n",
      "\n",
      "         [[ 0.4245]],\n",
      "\n",
      "         [[-0.4266]],\n",
      "\n",
      "         [[-0.5369]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3908]],\n",
      "\n",
      "         [[-0.9467]],\n",
      "\n",
      "         [[-0.1213]],\n",
      "\n",
      "         [[ 0.2436]],\n",
      "\n",
      "         [[ 0.0069]],\n",
      "\n",
      "         [[-0.8717]],\n",
      "\n",
      "         [[-0.2013]],\n",
      "\n",
      "         [[-0.2744]],\n",
      "\n",
      "         [[-0.1687]]],\n",
      "\n",
      "\n",
      "        [[[-0.1880]],\n",
      "\n",
      "         [[ 0.3923]],\n",
      "\n",
      "         [[ 0.5278]],\n",
      "\n",
      "         [[ 0.5584]],\n",
      "\n",
      "         [[ 0.4820]],\n",
      "\n",
      "         [[-0.1539]],\n",
      "\n",
      "         [[ 0.2396]],\n",
      "\n",
      "         [[-0.2228]],\n",
      "\n",
      "         [[ 0.2352]]],\n",
      "\n",
      "\n",
      "        [[[-0.4250]],\n",
      "\n",
      "         [[ 0.0445]],\n",
      "\n",
      "         [[-0.1903]],\n",
      "\n",
      "         [[-0.4388]],\n",
      "\n",
      "         [[ 0.6786]],\n",
      "\n",
      "         [[ 0.3648]],\n",
      "\n",
      "         [[-0.5352]],\n",
      "\n",
      "         [[-0.2713]],\n",
      "\n",
      "         [[ 0.3738]]],\n",
      "\n",
      "\n",
      "        [[[-0.6621]],\n",
      "\n",
      "         [[ 0.7478]],\n",
      "\n",
      "         [[-0.2228]],\n",
      "\n",
      "         [[ 0.1790]],\n",
      "\n",
      "         [[ 0.1454]],\n",
      "\n",
      "         [[-0.0412]],\n",
      "\n",
      "         [[-0.1560]],\n",
      "\n",
      "         [[-0.4285]],\n",
      "\n",
      "         [[ 0.2314]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8463]],\n",
      "\n",
      "         [[-0.1954]],\n",
      "\n",
      "         [[ 0.3726]],\n",
      "\n",
      "         [[ 0.0096]],\n",
      "\n",
      "         [[-0.1779]],\n",
      "\n",
      "         [[-0.3722]],\n",
      "\n",
      "         [[-0.2131]],\n",
      "\n",
      "         [[-0.5906]],\n",
      "\n",
      "         [[-0.5169]]],\n",
      "\n",
      "\n",
      "        [[[-0.6083]],\n",
      "\n",
      "         [[-0.2713]],\n",
      "\n",
      "         [[ 0.0522]],\n",
      "\n",
      "         [[-0.2438]],\n",
      "\n",
      "         [[ 0.0036]],\n",
      "\n",
      "         [[ 0.4560]],\n",
      "\n",
      "         [[-0.0693]],\n",
      "\n",
      "         [[ 0.1358]],\n",
      "\n",
      "         [[ 0.3626]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1996]],\n",
      "\n",
      "         [[-0.0967]],\n",
      "\n",
      "         [[ 0.2324]],\n",
      "\n",
      "         [[ 0.6500]],\n",
      "\n",
      "         [[ 0.6809]],\n",
      "\n",
      "         [[ 0.8633]],\n",
      "\n",
      "         [[-0.3687]],\n",
      "\n",
      "         [[ 0.6552]],\n",
      "\n",
      "         [[ 0.3154]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1492]],\n",
      "\n",
      "         [[ 0.7032]],\n",
      "\n",
      "         [[-0.1206]],\n",
      "\n",
      "         [[-0.0814]],\n",
      "\n",
      "         [[-0.6275]],\n",
      "\n",
      "         [[-0.4549]],\n",
      "\n",
      "         [[ 0.0999]],\n",
      "\n",
      "         [[-0.2357]],\n",
      "\n",
      "         [[-0.1237]]],\n",
      "\n",
      "\n",
      "        [[[-0.8807]],\n",
      "\n",
      "         [[ 0.2690]],\n",
      "\n",
      "         [[-0.6664]],\n",
      "\n",
      "         [[-0.4411]],\n",
      "\n",
      "         [[ 0.3557]],\n",
      "\n",
      "         [[ 0.1657]],\n",
      "\n",
      "         [[-0.4294]],\n",
      "\n",
      "         [[ 0.2870]],\n",
      "\n",
      "         [[ 0.1414]]],\n",
      "\n",
      "\n",
      "        [[[-0.3084]],\n",
      "\n",
      "         [[-0.7416]],\n",
      "\n",
      "         [[ 0.6159]],\n",
      "\n",
      "         [[-0.5550]],\n",
      "\n",
      "         [[ 0.0296]],\n",
      "\n",
      "         [[-0.4406]],\n",
      "\n",
      "         [[-0.0784]],\n",
      "\n",
      "         [[ 0.4314]],\n",
      "\n",
      "         [[ 0.5243]]],\n",
      "\n",
      "\n",
      "        [[[-0.7118]],\n",
      "\n",
      "         [[ 0.0889]],\n",
      "\n",
      "         [[-0.0594]],\n",
      "\n",
      "         [[ 0.7816]],\n",
      "\n",
      "         [[ 0.3973]],\n",
      "\n",
      "         [[ 0.2098]],\n",
      "\n",
      "         [[-0.1648]],\n",
      "\n",
      "         [[-0.1050]],\n",
      "\n",
      "         [[ 0.1912]]],\n",
      "\n",
      "\n",
      "        [[[-0.3019]],\n",
      "\n",
      "         [[-0.0374]],\n",
      "\n",
      "         [[-0.0604]],\n",
      "\n",
      "         [[-0.3911]],\n",
      "\n",
      "         [[-0.2257]],\n",
      "\n",
      "         [[ 0.6630]],\n",
      "\n",
      "         [[ 0.3593]],\n",
      "\n",
      "         [[ 0.3045]],\n",
      "\n",
      "         [[-1.3150]]],\n",
      "\n",
      "\n",
      "        [[[-0.1002]],\n",
      "\n",
      "         [[-0.3461]],\n",
      "\n",
      "         [[-0.2491]],\n",
      "\n",
      "         [[ 0.7853]],\n",
      "\n",
      "         [[-0.0417]],\n",
      "\n",
      "         [[-0.3374]],\n",
      "\n",
      "         [[-0.5981]],\n",
      "\n",
      "         [[ 0.3692]],\n",
      "\n",
      "         [[ 0.1100]]],\n",
      "\n",
      "\n",
      "        [[[-0.2952]],\n",
      "\n",
      "         [[ 0.7096]],\n",
      "\n",
      "         [[-0.2045]],\n",
      "\n",
      "         [[-0.1959]],\n",
      "\n",
      "         [[ 0.8778]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[-0.9322]],\n",
      "\n",
      "         [[-0.4896]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2100]],\n",
      "\n",
      "         [[ 0.3776]],\n",
      "\n",
      "         [[ 0.5413]],\n",
      "\n",
      "         [[ 0.1131]],\n",
      "\n",
      "         [[-0.0147]],\n",
      "\n",
      "         [[-0.1869]],\n",
      "\n",
      "         [[-0.4576]],\n",
      "\n",
      "         [[ 0.2328]],\n",
      "\n",
      "         [[ 0.2016]]],\n",
      "\n",
      "\n",
      "        [[[-0.0548]],\n",
      "\n",
      "         [[-0.4337]],\n",
      "\n",
      "         [[ 0.5420]],\n",
      "\n",
      "         [[-0.2203]],\n",
      "\n",
      "         [[ 0.2060]],\n",
      "\n",
      "         [[ 0.3011]],\n",
      "\n",
      "         [[ 0.1631]],\n",
      "\n",
      "         [[-0.2162]],\n",
      "\n",
      "         [[-0.4331]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0946]],\n",
      "\n",
      "         [[-0.4173]],\n",
      "\n",
      "         [[-0.5253]],\n",
      "\n",
      "         [[ 0.9660]],\n",
      "\n",
      "         [[ 0.8692]],\n",
      "\n",
      "         [[-0.3033]],\n",
      "\n",
      "         [[-0.4452]],\n",
      "\n",
      "         [[ 0.1285]],\n",
      "\n",
      "         [[-0.1536]]],\n",
      "\n",
      "\n",
      "        [[[-0.8046]],\n",
      "\n",
      "         [[-0.4378]],\n",
      "\n",
      "         [[ 0.1646]],\n",
      "\n",
      "         [[ 0.3224]],\n",
      "\n",
      "         [[-0.4402]],\n",
      "\n",
      "         [[-0.2027]],\n",
      "\n",
      "         [[-0.6452]],\n",
      "\n",
      "         [[ 0.0667]],\n",
      "\n",
      "         [[ 0.2677]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4838]],\n",
      "\n",
      "         [[ 0.3389]],\n",
      "\n",
      "         [[ 0.4636]],\n",
      "\n",
      "         [[ 0.3308]],\n",
      "\n",
      "         [[ 0.4042]],\n",
      "\n",
      "         [[ 0.6269]],\n",
      "\n",
      "         [[ 0.0701]],\n",
      "\n",
      "         [[-0.2141]],\n",
      "\n",
      "         [[-0.7251]]],\n",
      "\n",
      "\n",
      "        [[[-0.0279]],\n",
      "\n",
      "         [[-0.0183]],\n",
      "\n",
      "         [[ 0.0998]],\n",
      "\n",
      "         [[ 0.0726]],\n",
      "\n",
      "         [[ 0.3346]],\n",
      "\n",
      "         [[-0.6198]],\n",
      "\n",
      "         [[-0.3257]],\n",
      "\n",
      "         [[-0.2406]],\n",
      "\n",
      "         [[-0.2342]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4814]],\n",
      "\n",
      "         [[ 0.5217]],\n",
      "\n",
      "         [[ 0.1549]],\n",
      "\n",
      "         [[-0.0256]],\n",
      "\n",
      "         [[-0.0942]],\n",
      "\n",
      "         [[ 0.4108]],\n",
      "\n",
      "         [[-0.0670]],\n",
      "\n",
      "         [[-0.4704]],\n",
      "\n",
      "         [[ 0.1218]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1969]],\n",
      "\n",
      "         [[-0.4113]],\n",
      "\n",
      "         [[ 0.5353]],\n",
      "\n",
      "         [[-0.4431]],\n",
      "\n",
      "         [[-1.3322]],\n",
      "\n",
      "         [[ 0.2216]],\n",
      "\n",
      "         [[-1.4128]],\n",
      "\n",
      "         [[ 0.0411]],\n",
      "\n",
      "         [[ 0.5733]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2712]],\n",
      "\n",
      "         [[ 0.0996]],\n",
      "\n",
      "         [[ 1.2787]],\n",
      "\n",
      "         [[-0.2626]],\n",
      "\n",
      "         [[-0.0055]],\n",
      "\n",
      "         [[ 0.5102]],\n",
      "\n",
      "         [[ 0.3490]],\n",
      "\n",
      "         [[-1.0544]],\n",
      "\n",
      "         [[ 0.7516]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1986]],\n",
      "\n",
      "         [[ 0.1053]],\n",
      "\n",
      "         [[ 0.1021]],\n",
      "\n",
      "         [[-0.7947]],\n",
      "\n",
      "         [[-0.2471]],\n",
      "\n",
      "         [[-0.6374]],\n",
      "\n",
      "         [[ 0.0243]],\n",
      "\n",
      "         [[-0.2300]],\n",
      "\n",
      "         [[ 0.0546]]],\n",
      "\n",
      "\n",
      "        [[[-0.0757]],\n",
      "\n",
      "         [[ 0.2937]],\n",
      "\n",
      "         [[ 0.6151]],\n",
      "\n",
      "         [[ 0.1530]],\n",
      "\n",
      "         [[ 0.6693]],\n",
      "\n",
      "         [[-0.7502]],\n",
      "\n",
      "         [[-0.0633]],\n",
      "\n",
      "         [[-0.7654]],\n",
      "\n",
      "         [[ 0.2010]]]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]]]),\n",
      "S=tensor([[[6.4800e-02],\n",
      "         [4.4828e-02],\n",
      "         [1.0055e-02],\n",
      "         ...,\n",
      "         [5.7807e-02],\n",
      "         [4.3466e-02],\n",
      "         [5.8697e-02]],\n",
      "\n",
      "        [[6.8599e-02],\n",
      "         [5.9618e-02],\n",
      "         [1.7653e-03],\n",
      "         ...,\n",
      "         [9.8770e-02],\n",
      "         [2.3255e-02],\n",
      "         [9.8182e-02]],\n",
      "\n",
      "        [[1.8616e-02],\n",
      "         [1.5653e-02],\n",
      "         [5.6034e-03],\n",
      "         ...,\n",
      "         [5.0640e-02],\n",
      "         [1.0544e-02],\n",
      "         [5.4874e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[8.5934e-03],\n",
      "         [2.8029e-02],\n",
      "         [4.1789e-02],\n",
      "         ...,\n",
      "         [5.4405e-02],\n",
      "         [5.1826e-03],\n",
      "         [2.1048e-02]],\n",
      "\n",
      "        [[1.5857e-02],\n",
      "         [9.5462e-02],\n",
      "         [3.6780e-03],\n",
      "         ...,\n",
      "         [1.1032e-02],\n",
      "         [4.0337e-03],\n",
      "         [1.0263e-01]],\n",
      "\n",
      "        [[4.2731e-02],\n",
      "         [4.9801e-05],\n",
      "         [3.0206e-02],\n",
      "         ...,\n",
      "         [3.1731e-02],\n",
      "         [1.6279e-02],\n",
      "         [6.1609e-02]]]),\n",
      "V=tensor([[[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]]]))\n",
      "W is: tensor([[[[ 6.4800e-02]],\n",
      "\n",
      "         [[ 4.4828e-02]],\n",
      "\n",
      "         [[-1.0055e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.7807e-02]],\n",
      "\n",
      "         [[ 4.3466e-02]],\n",
      "\n",
      "         [[-5.8697e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.8599e-02]],\n",
      "\n",
      "         [[ 5.9618e-02]],\n",
      "\n",
      "         [[-1.7653e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.8770e-02]],\n",
      "\n",
      "         [[ 2.3255e-02]],\n",
      "\n",
      "         [[ 9.8182e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.8616e-02]],\n",
      "\n",
      "         [[-1.5653e-02]],\n",
      "\n",
      "         [[ 5.6034e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.0640e-02]],\n",
      "\n",
      "         [[ 1.0544e-02]],\n",
      "\n",
      "         [[ 5.4874e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 8.5934e-03]],\n",
      "\n",
      "         [[ 2.8029e-02]],\n",
      "\n",
      "         [[-4.1789e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.4405e-02]],\n",
      "\n",
      "         [[-5.1826e-03]],\n",
      "\n",
      "         [[-2.1048e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.5857e-02]],\n",
      "\n",
      "         [[-9.5462e-02]],\n",
      "\n",
      "         [[ 3.6780e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1032e-02]],\n",
      "\n",
      "         [[ 4.0337e-03]],\n",
      "\n",
      "         [[ 1.0263e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.2731e-02]],\n",
      "\n",
      "         [[ 4.9801e-05]],\n",
      "\n",
      "         [[ 3.0206e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.1731e-02]],\n",
      "\n",
      "         [[ 1.6279e-02]],\n",
      "\n",
      "         [[-6.1609e-02]]]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]]]),\n",
      "S=tensor([[[0.0416],\n",
      "         [0.0749],\n",
      "         [0.0145],\n",
      "         ...,\n",
      "         [0.0863],\n",
      "         [0.0365],\n",
      "         [0.0462]],\n",
      "\n",
      "        [[0.0240],\n",
      "         [0.0031],\n",
      "         [0.0114],\n",
      "         ...,\n",
      "         [0.0542],\n",
      "         [0.0855],\n",
      "         [0.0632]],\n",
      "\n",
      "        [[0.0389],\n",
      "         [0.0628],\n",
      "         [0.0310],\n",
      "         ...,\n",
      "         [0.1583],\n",
      "         [0.0943],\n",
      "         [0.0405]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0141],\n",
      "         [0.0043],\n",
      "         [0.1049],\n",
      "         ...,\n",
      "         [0.0387],\n",
      "         [0.0090],\n",
      "         [0.0007]],\n",
      "\n",
      "        [[0.0535],\n",
      "         [0.0149],\n",
      "         [0.0086],\n",
      "         ...,\n",
      "         [0.1515],\n",
      "         [0.0706],\n",
      "         [0.0056]],\n",
      "\n",
      "        [[0.0704],\n",
      "         [0.0820],\n",
      "         [0.0264],\n",
      "         ...,\n",
      "         [0.0720],\n",
      "         [0.0601],\n",
      "         [0.0141]]]),\n",
      "V=tensor([[[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]]]))\n",
      "W is: tensor([[[[ 0.0416]],\n",
      "\n",
      "         [[ 0.0749]],\n",
      "\n",
      "         [[ 0.0145]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0863]],\n",
      "\n",
      "         [[-0.0365]],\n",
      "\n",
      "         [[-0.0462]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0240]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[-0.0114]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0542]],\n",
      "\n",
      "         [[ 0.0855]],\n",
      "\n",
      "         [[ 0.0632]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0389]],\n",
      "\n",
      "         [[ 0.0628]],\n",
      "\n",
      "         [[ 0.0310]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1583]],\n",
      "\n",
      "         [[-0.0943]],\n",
      "\n",
      "         [[ 0.0405]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0141]],\n",
      "\n",
      "         [[-0.0043]],\n",
      "\n",
      "         [[ 0.1049]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0387]],\n",
      "\n",
      "         [[ 0.0090]],\n",
      "\n",
      "         [[ 0.0007]]],\n",
      "\n",
      "\n",
      "        [[[-0.0535]],\n",
      "\n",
      "         [[ 0.0149]],\n",
      "\n",
      "         [[-0.0086]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1515]],\n",
      "\n",
      "         [[ 0.0706]],\n",
      "\n",
      "         [[ 0.0056]]],\n",
      "\n",
      "\n",
      "        [[[-0.0704]],\n",
      "\n",
      "         [[-0.0820]],\n",
      "\n",
      "         [[-0.0264]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0720]],\n",
      "\n",
      "         [[ 0.0601]],\n",
      "\n",
      "         [[ 0.0141]]]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[[[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[ 1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         [[-1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[-1.]]]]),\n",
      "S=tensor([[[0.0339],\n",
      "         [0.0318],\n",
      "         [0.0284],\n",
      "         ...,\n",
      "         [0.1170],\n",
      "         [0.1297],\n",
      "         [0.0623]],\n",
      "\n",
      "        [[0.0319],\n",
      "         [0.0265],\n",
      "         [0.0731],\n",
      "         ...,\n",
      "         [0.0080],\n",
      "         [0.0218],\n",
      "         [0.0295]],\n",
      "\n",
      "        [[0.1077],\n",
      "         [0.1024],\n",
      "         [0.0655],\n",
      "         ...,\n",
      "         [0.0372],\n",
      "         [0.0369],\n",
      "         [0.0260]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0100],\n",
      "         [0.0874],\n",
      "         [0.0058],\n",
      "         ...,\n",
      "         [0.0656],\n",
      "         [0.0067],\n",
      "         [0.0691]],\n",
      "\n",
      "        [[0.0353],\n",
      "         [0.1156],\n",
      "         [0.0464],\n",
      "         ...,\n",
      "         [0.1561],\n",
      "         [0.0847],\n",
      "         [0.1426]],\n",
      "\n",
      "        [[0.0798],\n",
      "         [0.0549],\n",
      "         [0.1093],\n",
      "         ...,\n",
      "         [0.1613],\n",
      "         [0.0260],\n",
      "         [0.0619]]]),\n",
      "V=tensor([[[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]]]))\n",
      "W is: tensor([[[[ 0.0339]],\n",
      "\n",
      "         [[ 0.0318]],\n",
      "\n",
      "         [[-0.0284]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1170]],\n",
      "\n",
      "         [[ 0.1297]],\n",
      "\n",
      "         [[-0.0623]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0319]],\n",
      "\n",
      "         [[-0.0265]],\n",
      "\n",
      "         [[ 0.0731]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0080]],\n",
      "\n",
      "         [[ 0.0218]],\n",
      "\n",
      "         [[ 0.0295]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1077]],\n",
      "\n",
      "         [[-0.1024]],\n",
      "\n",
      "         [[-0.0655]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0372]],\n",
      "\n",
      "         [[-0.0369]],\n",
      "\n",
      "         [[ 0.0260]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0100]],\n",
      "\n",
      "         [[-0.0874]],\n",
      "\n",
      "         [[-0.0058]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0656]],\n",
      "\n",
      "         [[-0.0067]],\n",
      "\n",
      "         [[ 0.0691]]],\n",
      "\n",
      "\n",
      "        [[[-0.0353]],\n",
      "\n",
      "         [[ 0.1156]],\n",
      "\n",
      "         [[ 0.0464]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1561]],\n",
      "\n",
      "         [[-0.0847]],\n",
      "\n",
      "         [[ 0.1426]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0798]],\n",
      "\n",
      "         [[-0.0549]],\n",
      "\n",
      "         [[-0.1093]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1613]],\n",
      "\n",
      "         [[ 0.0260]],\n",
      "\n",
      "         [[-0.0619]]]])\n"
     ]
    }
   ],
   "source": [
    "maml = Meta(update_lr = 0.1, meta_lr = 0.001, n_way = 5, k_spt = 1, k_qry = 15, task_num = 32, update_step = 5, update_step_test = 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64c1f4f4-e3b8-4a72-9b0d-e0c328a1732c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits are: tensor([[inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf],\n",
      "        [inf, inf, inf, inf, inf]], grad_fn=<PowBackward0>)\n",
      "loss is: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/torch/autograd/__init__.py:303: UserWarning: Error detected in LogSoftmaxBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.6_2/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.6_2/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.6_2/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.6_2/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.6_2/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/pv/gljhsnfs4bs254zbp4lm1y4m0000gq/T/ipykernel_6811/1128340290.py\", line 7, in <module>\n",
      "    accs = maml(x_spt, y_spt, x_qry, y_qry)\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/var/folders/pv/gljhsnfs4bs254zbp4lm1y4m0000gq/T/ipykernel_6811/2440947921.py\", line 50, in forward\n",
      "    loss = F.cross_entropy(logits, y_spt[i])\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/torch/nn/functional.py\", line 3029, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      " (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:119.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'LogSoftmaxBackward0' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m x_spt, y_spt, x_qry, y_qry \u001b[38;5;241m=\u001b[39m db_train\u001b[38;5;241m.\u001b[39mnext()\n\u001b[1;32m      5\u001b[0m x_spt, y_spt, x_qry, y_qry \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(x_spt)\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mfrom_numpy(y_spt)\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mfrom_numpy(x_qry)\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mfrom_numpy(y_qry)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m accs \u001b[38;5;241m=\u001b[39m \u001b[43mmaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_spt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_spt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_qry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_qry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep:\u001b[39m\u001b[38;5;124m'\u001b[39m, step, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mtraining acc:\u001b[39m\u001b[38;5;124m'\u001b[39m, accs)\n",
      "File \u001b[0;32m~/Photonic_computing/photonics_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[51], line 53\u001b[0m, in \u001b[0;36mMeta.forward\u001b[0;34m(self, x_spt, y_spt, x_qry, y_qry)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# avoid very large gradients killing everything\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrads are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(grad)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Photonic_computing/photonics_env/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'LogSoftmaxBackward0' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "\n",
    "for step in range(num_epochs):\n",
    "    x_spt, y_spt, x_qry, y_qry = db_train.next()\n",
    "    x_spt, y_spt, x_qry, y_qry = torch.from_numpy(x_spt).to(device), torch.from_numpy(y_spt).to(device), torch.from_numpy(x_qry).to(device), torch.from_numpy(y_qry).to(device)\n",
    "\n",
    "    accs = maml(x_spt, y_spt, x_qry, y_qry)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print('step:', step, '\\ttraining acc:', accs)\n",
    "\n",
    "    if step % 500 == 0 and step > 0:\n",
    "        accs = []\n",
    "        for _ in range(1000//32):\n",
    "            # test\n",
    "            x_spt, y_spt, x_qry, y_qry = db_train.next('test')\n",
    "            x_spt, y_spt, x_qry, y_qry = torch.from_numpy(x_spt).to(device), torch.from_numpy(y_spt).to(device), \\\n",
    "                                        torch.from_numpy(x_qry).to(device), torch.from_numpy(y_qry).to(device)\n",
    "            # split to single task each time\n",
    "            for x_spt_one, y_spt_one, x_qry_one, y_qry_one in zip(x_spt, y_spt, x_qry, y_qry):\n",
    "                test_acc = maml.finetuning(x_spt_one, y_spt_one, x_qry_one, y_qry_one)\n",
    "                accs.append(test_acc)\n",
    "        accs = np.array(accs).mean(axis = 0).astype(np.float16)\n",
    "        print('Test acc:', accs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a814108c-5748-497a-837a-3adb8b59d7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
